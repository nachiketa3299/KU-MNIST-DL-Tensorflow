9 ~ 13을 통해서 레이어 사이즈를 바꿔가며 tA를 측정해 보았는데, 레이어 수가 늘어난다고 무조건 좋은 것이 아니었다. 아무래도 해당 데이터를 설명하는 층위가 늘어나면 늘어날수록, 트레이닝 셋에 과적합될 위험성이 높아지는 것 같다.
원래라면 9 ~ 13 중에서 테스트 셋에 대해서 가장 높은 정확도를 보이는 10번을 바탕으로 하이퍼파라미터를 튜닝해 나가는 것이 맞겠지만 과제에서 제시된 최대 레이어보다 한 단계 더 높은 모델을 만들어 보고 싶어서 11번 모델을 바탕으로 튜닝해 나가 보기로 했다.
그 다음 바꿔본 요소는 가중치 초기화 방법이다. 14와 15가 그 결과인데, HE가 1%나 더 좋은 성능을 보였다.
16부터 18에서는 Weight Decay를 추가해 주었다. 0.1부터 0.001까지 줄여 나가는 방법을 택해 보았는데, 0.1의 경우 눈에 띄게 정확도가 낮았다. 0.01과 0.001 사이에서 0.001을 택하기로 했다.
그 다음은 Dropout을 튜닝했다. 19 ~ 21이 그 결과인데, 여기서 Dropout의 값은 뉴런이 제거될 확률이다. (keep_prob과 정 반대이다.) 0.2로 주었을때의 성능이 가장 높게 나왔다.
Dropout까지 튜닝하고 나서 95%정도의 정확도를 얻었지만 목표치에는 많이 못 미쳤다.
따라서 100이었던 배치 사이즈를 200으로 바꾸어 조금 더 데이터를 일반화해서 받아보려고 시도했다.
200으로 바꾼 것이 22번인데, 바꾸자 마자 96%로 정확도가 상승했다.
그 후에 여러가지 파라미터들을 조금씩 수정했다.
첫번째로 다시 Dropout확률을 수정해 보았는데, 0.2 근방에서 조금씩 바꿔보았지만 0,2가 최적이었다. 힘빠졌다.
그 다음 26에서 Optimizer도 바꾸어 보았지만 오히려 극적으로 정확도가 낮아졌다. 아무래도 손글씨 인식에서는 adadelta optimizer는 맞지 않는 것 같다.
27부터 31까지 Weight Decay를 큰 단위로 바꾸어 보았는데, 0.0001에서 최적의 성능을 보였다. 솔직히 너무 작은 값이어서 가중치 매트릭스의 L2 Norm합이 너무 적게 반영이 된 것이 아닌가 했지만, 어쨌든 목표치인 97에 도달하긴 하였다. (31번)

과제의 요건에는 도달했지만 98을 넘어보고 싶어서 바꾸어 보지 않았던 Learning Rate를 건드려 본 것이 32부터 35이다. 