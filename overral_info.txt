preset	batch_size	activation_function	#_of_layers	layer_size	training_epoch	weight_init	optimizer	weight_decay	dropout	training_time	early_stopping	val_maxAcc	timestamp
1	200	relu	3	(300, 200)	30	None	adam	None	None	0.7543416619300842	27	0.9855999946594238	1600329251
2	200	relu	4	(200, 200, 200)	100	None	adam	None	None	2.348506780465444	86	0.9865999817848206	1600329297
3	200	relu	4	(600, 600, 800)	100	None	adam	None	None	8.371021564801534	60	0.9868000149726868	1600329440
4	200	relu	4	(200, 200, 200)	100	None	adam	None	None	2.3552965402603148	60	0.9850000143051147	1600329944
5	200	relu	4	(200, 200, 200)	100	he	adadelta	None	None	3.182493595282237	100	0.8392000198364258	1600330086
6	200	relu	4	(200, 200, 200)	100	he	adam	0.01	None	2.3740461627642313	67	0.9639999866485596	1600330279
7	200	relu	4	(200, 200, 200)	100	he	adam	0.01	None	2.280578108628591	91	0.9452000260353088	1600330423
8	200	relu	4	(200, 200, 200)	100	he	adam	0.01	0.2	2.90297402938207	85	0.9074000120162964	1600330561
1	200	relu	3	(300, 200)	30	None	adam	0	0	0.7219958066940307	27	0.9850000143051147	1600410033
2	200	relu	4	(200, 200, 200)	100	None	adam	0	0	2.3566701571146647	92	0.9855999946594238	1600410077
3	200	relu	4	(600, 600, 800)	100	None	adam	0	0	8.335224743684133	54	0.9872000217437744	1600410220
4	200	relu	4	(200, 200, 200)	100	None	adam	0	0	2.3471771478652954	53	0.9843999743461609	1600410722
5	200	relu	4	(200, 200, 200)	100	he	adadelta	0	0	3.1616779367129006	100	0.8396000266075134	1600410865
6	200	relu	4	(200, 200, 200)	100	he	adam	0.01	0	2.408017865816752	100	0.9332000017166138	1600411056
7	200	relu	4	(200, 200, 200)	100	he	adam	0.01	0	2.3498168150583902	95	0.9088000059127808	1600411202
8	200	relu	4	(200, 200, 200)	100	he	adam	0.01	0.2	2.9047884623209637	57	0.8640000224113464	1600411345
