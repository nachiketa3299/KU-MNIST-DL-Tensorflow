preset	batch_size	activation_function	#_of_layers	layer_size	training_epoch	weight_init	optimizer	weight_decay	dropout	training_time	early_stopping	val_maxAcc	timestamp
32	200	relu	5	(300, 200, 200, 200)	100	he	adam	0.01	0.0001	0.2	4.110672092437744	19	0.9453999996185303	1600544417
33	200	relu	5	(300, 200, 200, 200)	100	he	adam	0.1	0.0001	0.2	4.34507182041804	1	0.11259999871253967	1600544666
34	200	relu	5	(300, 200, 200, 200)	100	he	adam	1	0.0001	0.2	4.380726746718088	7	0.11259999871253967	1600544929
35	200	relu	5	(300, 200, 200, 200)	100	he	adam	0.0001	0.0001	0.2	5.1025887330373125	87	0.9779999852180481	1600545195
36	200	relu	4	(300, 200, 200)	100	he	adam	0.001	0.0001	0.2	3.7139614383379618	87	0.9811999797821045	1600546465
37	200	relu	4	(300, 200, 200)	100	he	adam	0.001	0.01	0.2	3.6349635481834413	78	0.9124000072479248	1600546968
